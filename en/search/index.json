[{"content":"1. Why Gas Optimization Matters When developing smart contracts on Ethereum, Gas is an unavoidable concept. It is neither merely a ‚Äútransaction fee‚Äù nor just a temporary cost caused by network congestion. Instead, it represents a long-term constraint on contract design quality.\nMany developers start paying attention to Gas only after encountering situations such as:\nExceptionally high deployment costs Frequent ‚ÄúOut of Gas‚Äù errors when users call certain functions Significant cost differences between different implementations of the same functionality These issues usually do not stem from business logic mistakes, but rather from a lack of intuition about the EVM cost model.\n1.1 The Two Meanings of Gas Before discussing optimization, it is important to distinguish two commonly confused concepts:\ngas used: the number of Gas units actually consumed when executing a transaction gas price: the price you are willing to pay per unit of Gas Contract code can only affect gas used, not gas price.\nThis means:\nNetwork congestion increases gas price, but does not change a contract‚Äôs gas usage A poorly designed contract is expensive under any network conditions When gas price is high, inefficiencies in contract design become even more costly Therefore, Gas optimization is not about ‚Äúbetting on network conditions,‚Äù but about minimizing the number of Gas units consumed per execution.\n1.2 Why ‚ÄúFunctionally Correct‚Äù Does Not Mean ‚ÄúCost-Efficient‚Äù In traditional software development, as long as the program works correctly, performance issues can often be optimized later. In smart contracts, however, performance is cost.\nEven a contract that:\nHas no security vulnerabilities Implements the intended functionality correctly Passes all tests can still become impractical because:\nCertain functions are too expensive to execute on-chain Transaction failure rates increase during peak periods Users pay unnecessary costs for the same functionality over time These problems are rarely caused by ‚Äúwrong code,‚Äù but by ignoring structural Gas costs at the design stage.\n1.3 The Goals of Gas Optimization Gas optimization is not about achieving the absolute lowest cost, but about serving three practical goals:\nReducing long-term usage costs For frequently called functions, saving even a few hundred Gas can add up significantly over time.\nImproving transaction success rates More predictable Gas usage reduces the risk of running out of Gas in complex execution paths.\nImproving cost predictability Making it easier for callers to estimate required Gas and reducing uncertainty.\nThis is why Gas optimization should usually focus on:\nHigh-frequency execution paths Core business logic Functions where users directly pay execution costs 1.4 When Not to Over-Optimize It is important to recognize that Gas optimization has diminishing returns.\nThe following are usually not worth doing:\nIntroducing complex and obscure code to save a trivial amount of Gas Applying heavy micro-optimizations to low-frequency or cold paths Sacrificing safety checks or readability for marginal gains A reasonable principle is:\nWrite safe, clear, and maintainable code first, then optimize only where it truly matters.\nIn most cases, understanding and avoiding high-cost structures is more valuable than memorizing isolated tricks.\n1.5 What Comes Next The following sections start from the fundamentals:\nWhat the EVM actually charges Gas for Which instructions are expensive and which are almost negligible Why storage access is the core driver of Gas costs Once you understand these principles, later optimization practices will feel natural rather than rule-based.\n2. The Gas Cost Model Before diving into specific optimization techniques, it is essential to build a clear cost intuition: not all EVM operations cost the same.\nMany Solidity constructs that appear ‚Äúsimple‚Äù consume large amounts of Gas because they trigger expensive low-level instructions.\nThe goal of this section is simple: Know which operations should be avoided or minimized, and which can be largely ignored.\n2.1 Instruction-Level Pricing The EVM is a stack-based virtual machine. Solidity code is compiled into a sequence of EVM opcodes such as:\nArithmetic and comparison instructions like ADD, SUB, LT Memory operations like MLOAD, MSTORE Storage operations like SLOAD, SSTORE External calls like CALL, DELEGATECALL Gas is charged entirely at the instruction level, not at the Solidity syntax level. This means:\nA single line of Solidity can compile into multiple opcodes Different-looking code can compile into very different instruction sequences Gas differences come from instruction types and counts, not from code length In essence, Gas optimization means one thing:\nExecute expensive instructions fewer times.\n2.2 Cost Tiers From a cost perspective, EVM operations can be roughly grouped (from cheapest to most expensive):\nPure computation (arithmetic, comparisons, bitwise operations) Memory reads and writes Calldata reads Storage reads (SLOAD) Storage writes (SSTORE) External calls and large data returns The key takeaway is:\nStorage operations are far more expensive than most computations.\nThis is why so many Gas optimizations ultimately converge on the same goal: reducing storage access.\n2.3 What Storage Is and Why It Is Expensive In Solidity, all state variables are stored in storage. From the EVM‚Äôs perspective, storage is a massive key‚Äìvalue store:\nKey: storage slot index Value: 32 bytes of data Storage has several defining characteristics:\nData is persistent It affects the global state trie All full nodes must agree on its contents Every storage write modifies the global state, which is the fundamental reason storage operations are so costly.\n2.4 The Real Cost of Reading Storage When you read a state variable, for example:\nuint256 x = count; the key instruction is SLOAD.\nSince EIP-2929, SLOAD has two cost modes:\nCold access: the first access to a storage slot in a transaction Warm access: subsequent accesses to the same slot in the same transaction Intuitively:\nThe first read ‚Äúbrings the value in‚Äù Subsequent reads are cheaper, but still far more expensive than memory or computation Even warm SLOAD is significantly more expensive than arithmetic or memory access.\n2.5 Why Writing Storage Is Among the Most Expensive Operations When you modify a state variable, such as:\ncount = count + 1; this is not a simple increment, but a full read‚Äìmodify‚Äìwrite sequence:\nSLOAD ‚Äì read the old value Perform the addition SSTORE ‚Äì write the new value The cost of SSTORE depends on the state transition:\nZero ‚Üí non-zero: most expensive Non-zero ‚Üí non-zero: less expensive Non-zero ‚Üí zero: cheaper and may trigger a refund These rules are designed to encourage contracts to free unused storage.\n2.6 What a Single Line of Solidity Really Does Consider this common statement:\ncount += 1; From Solidity‚Äôs perspective, it is trivial. From the EVM‚Äôs perspective, it usually means:\nOne SLOAD One arithmetic instruction One SSTORE If you repeat it in the same function:\ncount += 1; count += 1; count += 1; you are not performing three additions, but triggering:\n3 SLOADs 3 SSTOREs This is how Gas costs escalate quickly.\n2.7 A Key Intuition At this point, a crucial intuition should be clear:\nArithmetic and logic are rarely the bottleneck Storage reads and writes dominate Gas costs Repeated access to the same storage slot is one of the most common and overlooked sources of waste Once this intuition is established, later optimizations‚Äîcaching, write merging, storage packing‚Äîwill feel obvious rather than magical.\n3. Reducing Storage Reads and Writes After understanding the EVM cost model, the priority of Gas optimization becomes very clear:\nAny reduction in storage access almost certainly leads to significant Gas savings.\nMost Gas optimization techniques revolve around a single core objective:\nMake SLOAD and SSTORE execute as few times as possible.\n3.1 Why Storage Optimization Has the Highest ROI As discussed earlier:\nArithmetic operations are very cheap Memory operations are moderately priced Storage writes are among the most expensive operations This implies:\nEliminating a single SSTORE is often more valuable than optimizing dozens of arithmetic instructions Optimizing storage access typically yields order-of-magnitude improvements In practice, Gas optimization should follow this order:\nFirst, reduce storage reads and writes Then consider secondary optimizations such as loops, parameters, and bit-level tricks 3.2 Caching Repeated Reads: Turning Multiple SLOADs into One One of the most common and easily overlooked inefficiencies is reading the same state variable multiple times within a function.\nFor example:\nfunction increment() external { require(count \u0026lt; max, \u0026#34;too large\u0026#34;); count = count + 1; emit Updated(count); } In this code, count is actually read multiple times:\nOnce in the require Once during the increment Once again when emitting the event A more efficient version caches it in a local variable:\nfunction increment() external { uint256 c = count; require(c \u0026lt; max, \u0026#34;too large\u0026#34;); c = c + 1; count = c; emit Updated(c); } The result is:\nStorage is read only once Storage is written only once All intermediate operations happen in memory This change barely affects readability, yet can significantly reduce Gas usage.\n3.3 Merging Writes: Avoiding Repeated SSTOREs Another common issue is writing to the same storage variable multiple times within a single function.\nFor example:\nfunction update(uint256 x) external { value += x; if (x \u0026gt; threshold) { value += bonus; } } Although the logic is clear, this code may trigger multiple SSTOREs on value.\nA better approach is to perform all calculations in memory first:\nfunction update(uint256 x) external { uint256 v = value; v += x; if (x \u0026gt; threshold) { v += bonus; } value = v; } The guiding principle is:\nDo not repeatedly write to storage inside conditional logic‚Äîcompute first, then write back once.\n3.4 Storage Pitfalls Inside Loops Loops are where storage inefficiencies are most easily amplified.\nConsider the following implementation:\nfunction sum(uint256[] calldata arr) external { for (uint256 i = 0; i \u0026lt; arr.length; i++) { total += arr[i]; } } In this loop:\ntotal is read and written on every iteration For an array of length n, this triggers n SLOADs and n SSTOREs A more efficient version is:\nfunction sum(uint256[] calldata arr) external { uint256 t = total; uint256 len = arr.length; for (uint256 i = 0; i \u0026lt; len; i++) { t += arr[i]; } total = t; } This small change has an impact that grows linearly with the array length.\n3.5 Design Principles for Avoiding Storage Writes in Loops When designing contracts, try to avoid patterns like:\nWriting to storage inside unbounded loops Updating state on every iteration Letting loop bounds be fully controlled by external input Better alternatives include:\nComputing results in memory and committing once Designing batch interfaces with strict per-call limits Moving complex computation off-chain and only verifying results on-chain These are not mere ‚Äúcoding tricks,‚Äù but structural design decisions that should be made early.\n3.6 Using Mappings Instead of Arrays In contracts, the choice between arrays and mappings is not just a style preference‚Äîit reflects a fundamental cost model difference:\nArray operations like search, deduplication, or deletion often require traversal, with O(n) cost and unbounded loops Mapping access is direct by key, close to O(1), making it more predictable and Gas-efficient Case 1: Membership Checks (contains) Not recommended: storing members in an array and searching on-chain\naddress[] public members; function isMember(address a) public view returns (bool) { for (uint256 i = 0; i \u0026lt; members.length; i++) { if (members[i] == a) return true; } return false; } Problems:\nRequires traversal on every check Cost grows with the number of members Worst case can run out of Gas Recommended: use a mapping for existence checks\nmapping(address =\u0026gt; bool) public isMember; function addMember(address a) external { isMember[a] = true; } function removeMember(address a) external { isMember[a] = false; } Advantages:\nO(1) existence checks Predictable cost No loops required Case 2: Enumerable Sets (O(1) checks + enumeration) Some applications require both:\nO(1) membership checks Enumeration of all members (e.g., for frontends) A common pattern is mapping + array:\nmapping(address =\u0026gt; bool) public isMember; address[] public memberList; mapping(address =\u0026gt; uint256) private indexPlusOne; // index + 1, 0 means absent function addMember(address a) external { if (indexPlusOne[a] != 0) return; isMember[a] = true; memberList.push(a); indexPlusOne[a] = memberList.length; } function removeMember(address a) external { uint256 idxPlusOne = indexPlusOne[a]; if (idxPlusOne == 0) return; uint256 idx = idxPlusOne - 1; uint256 last = memberList.length - 1; if (idx != last) { address lastAddr = memberList[last]; memberList[idx] = lastAddr; indexPlusOne[lastAddr] = idx + 1; } memberList.pop(); indexPlusOne[a] = 0; isMember[a] = false; } Explanation:\nThe mapping handles O(1) checks and indexing The array handles enumeration Deletion uses swap-and-pop to avoid O(n) shifts Note:\nThis introduces extra storage (index mapping) In exchange, operation cost becomes predictable and bounded‚Äîusually well worth it When Arrays Are the Wrong Choice If you find yourself doing any of the following with arrays, consider switching to mappings:\nMembership checks Deduplication Deleting arbitrary elements Preventing duplicate inserts Any traversal driven by unbounded user input In short:\nArrays are for ordered data and index-based access. Mappings are for lookup, deduplication, and existence checks. When searching or deleting is involved, mappings are usually cheaper and safer.\n4. Data Location and Function Interface Design After reducing unnecessary storage access, the next category of high‚Äìvalue optimizations lies in function interface design, especially:\nParameter data locations Function visibility These choices usually do not affect business logic, but they directly influence:\nWhether unnecessary data copying occurs Whether extra ABI encoding/decoding is triggered Which execution path the EVM takes Well-designed interfaces often provide low-risk, high-return Gas optimizations.\n4.1 Cost Intuition for Data Locations In Solidity, reference types (arrays, structs, strings, bytes) must explicitly or implicitly specify a data location:\ncalldata: read-only, located in call data memory: read‚Äìwrite, exists during function execution storage: persistent, stored on-chain From a cost perspective, a simple intuition applies:\nReading calldata: cheap Reading/writing memory: moderate Reading/writing storage: expensive A fundamental rule follows:\nPrefer calldata over memory, and memory over storage whenever possible.\n4.2 external Functions and calldata For functions that are only called externally, the recommended pattern is:\nfunction process(uint256[] calldata data) external { // use data } Reasons:\nParameters of external functions naturally live in calldata No need to copy data into memory For large arrays or complex structs, the savings can be substantial By contrast:\nfunction process(uint256[] memory data) public { // use data } Even if data is not modified, the compiler must:\nCopy the entire calldata payload into memory Pay the additional Gas cost for that copy 4.3 When memory Is Required The limitation of calldata is clear: it is read-only.\nIf a function needs to:\nModify array contents Sort Deduplicate Dynamically construct new arrays then memory is required.\nExample:\nfunction normalize(uint256[] calldata data) external returns (uint256[] memory) { uint256[] memory result = new uint256[](data.length); for (uint256 i = 0; i \u0026lt; data.length; i++) { result[i] = data[i] / 2; } return result; } Key points:\nInput parameters use calldata to avoid copying Output uses memory because it must be writable This is a very common and sensible pattern.\n4.4 Gas Implications of Function Visibility Function visibility affects not only accessibility, but also Gas cost.\nA useful intuition:\nexternal: reads parameters directly from calldata, cheapest public: parameters are copied into memory, more expensive internal: often inlined or called via jump, very cheap private: similar to internal, but limited to the contract An important conclusion:\npublic is not a universally optimal ‚Äúinside-and-outside‚Äù choice.\n4.5 External Entry + Internal Implementation A highly recommended pattern in real projects is:\nExpose external functions Move core logic into internal functions Example:\nfunction update(uint256 x) external { _update(x); } function _update(uint256 x) internal { // core logic } Benefits:\nExternal calls use calldata with minimal copying Internal calls are extremely cheap One shared implementation for both internal and external usage This pattern has almost no downsides and avoids many hidden Gas costs.\n4.6 Why You Should Avoid Calling this A subtle but expensive anti-pattern is:\nthis.update(x); Even if update is defined in the same contract, this triggers:\nA full external call ABI encoding and decoding A CALL opcode execution Consequences:\nHigher Gas cost More complex execution path Potential reentrancy risks If you find yourself using this.foo(), it usually means:\nLogic is poorly structured Internal abstraction is insufficient The correct refactor is to extract logic into an internal function and call it directly.\n4.7 A Comparison Example Less efficient approach:\nfunction foo(uint256[] memory data) public { // use data } function bar(uint256[] memory data) public { foo(data); } Recommended approach:\nfunction foo(uint256[] calldata data) external { _foo(data); } function bar(uint256[] calldata data) external { _foo(data); } function _foo(uint256[] calldata data) internal { // use data } Why the second approach is better:\nAvoids repeated parameter copying Centralizes logic Uses cheaper internal calls 5. State Layout Design: Storage Packing So far, most optimizations have focused on how state variables are accessed. This section addresses a more structural question:\nHow state variables are laid out in storage.\nA significant amount of Gas waste comes not from frequent access, but from inefficient state layout, leading to:\nMore storage slots than necessary Extra SLOAD / SSTORE operations Amplified long-term costs 5.1 Storage Slots and 32-Byte Alignment From the EVM‚Äôs perspective, storage is organized in 32-byte (256-bit) slots.\nSolidity places state variables into slots in declaration order:\nVariables smaller than 32 bytes may share a slot If remaining space is insufficient, a new slot is used Once a slot is full, no more variables are packed into it This mechanism is known as storage packing.\n5.2 A Basic Packing Example Consider:\nuint128 a; uint128 b; uint256 c; Each uint128 uses 16 bytes, so a and b share one slot. c occupies its own slot. Total: 2 slots.\nNow change the order:\nuint128 a; uint256 c; uint128 b; Result:\na uses the first 16 bytes of slot 0 c occupies slot 1 entirely b cannot fit in slot 0 and goes to slot 2 Total: 3 slots‚Äîone extra slot used purely due to ordering.\n5.3 Why Slot Count Directly Affects Gas Every additional storage slot increases long-term cost:\nMore slots read ‚Üí more SLOADs More slots written ‚Üí more SSTOREs Higher cost when copying or updating structs This is especially important in:\nHigh-frequency functions Struct-heavy designs Long-lived contracts 5.4 Smaller Types Are Not Always Cheaper Important caveat:\nUsing types smaller than 256 bits does not automatically save Gas Savings only occur if packing actually happens Example:\nuint128 a; uint256 b; Even though a is smaller, it still occupies its own slot because b follows it.\nType choice and declaration order must be considered together.\n5.5 When Storage Packing Is Worth the Effort Not every contract needs aggressive packing.\nStorage packing is most valuable when:\nThere are many state variables Structs are used extensively State is frequently read and written The contract has a long operational lifetime For small, rarely-used contracts, the benefit may be marginal.\n6. Loops and Batch Processing From the previous sections, one pattern should already be clear:\nStorage access is expensive Loops amplify that cost linearly Loops themselves are not inherently bad, but unbounded or poorly designed loops almost always become a Gas problem.\n6.1 Why Loops Easily Become Gas Black Holes From the EVM‚Äôs perspective, a loop is nothing special‚Äîit simply:\nRepeats a sequence of instructions Pays the full instruction cost on every iteration If the loop body contains:\nStorage reads or writes Expensive computations External calls then Gas usage grows linearly with the iteration count.\nThe risk becomes severe when the loop length is controlled by external input.\n6.2 Caching Array Length and Intermediate Results A very common and basic optimization is caching an array‚Äôs length.\nFor example:\nfunction sum(uint256[] calldata arr) external { uint256 s = 0; for (uint256 i = 0; i \u0026lt; arr.length; i++) { s += arr[i]; } } Although arr.length looks trivial, it is re-read on every loop condition check.\nA better version is:\nfunction sum(uint256[] calldata arr) external { uint256 s = 0; uint256 len = arr.length; for (uint256 i = 0; i \u0026lt; len; i++) { s += arr[i]; } } This optimization saves little Gas per iteration, but the difference becomes noticeable in large arrays or high-frequency calls.\n6.3 Avoid Writing to Storage Inside Loops As emphasized earlier, writing to storage inside loops is extremely expensive.\nThe principle can be summarized as:\nDo calculations inside loops, but commit results to storage only once, outside the loop.\n6.4 Using unchecked Safely Since Solidity 0.8, integer arithmetic includes overflow checks by default. This is valuable for safety, but it also introduces extra Gas cost.\nA typical example is a loop counter:\nfor (uint256 i = 0; i \u0026lt; len; i++) { // ... } If you can guarantee that:\ni will never approach type(uint256).max The loop has a strict upper bound then you can use:\nfor (uint256 i = 0; i \u0026lt; len; ) { // ... unchecked { i++; } } The Gas savings are smaller than storage optimizations, but still measurable in large loops.\n6.5 Avoid Unbounded Loops When designing contract interfaces, try to avoid:\nLoop bounds fully controlled by user input Loops without explicit upper limits For example:\nfunction process(uint256[] calldata items) external { for (uint256 i = 0; i \u0026lt; items.length; i++) { // ... } } If items.length is unrestricted, callers can pass extremely large arrays, causing:\nTransaction failure due to Out of Gas Practical unavailability of the function Common improvements include:\nEnforcing a maximum length Splitting work across multiple transactions Providing paginated or cursor-based APIs 6.6 Trade-offs in Batch Design Batch processing is often used to reduce transaction count and amortize fixed costs, but it is not free.\nAdvantages:\nFewer external calls Amortized function entry and validation cost Risks:\nUnpredictable per-transaction Gas usage Higher likelihood of Out of Gas Harder Gas estimation Therefore, batch interfaces should usually provide:\nA clear per-call upper bound Predictable worst-case cost Well-defined failure behavior 6.7 A Batch Processing Example Not recommended:\nfunction batchUpdate(uint256[] calldata ids) external { for (uint256 i = 0; i \u0026lt; ids.length; i++) { update(ids[i]); } } Improved version:\nuint256 constant MAX_BATCH = 100; function batchUpdate(uint256[] calldata ids) external { uint256 len = ids.length; require(len \u0026lt;= MAX_BATCH, \u0026#34;too many items\u0026#34;); for (uint256 i = 0; i \u0026lt; len; ) { _update(ids[i]); unchecked { i++; } } } The key improvement is not raw Gas savings, but:\nControlled cost Predictable behavior A more user-friendly interface 7. Error Handling and Bytecode Size Optimization Gas optimization discussions often focus on the ‚Äúsuccessful execution path,‚Äù while overlooking failure paths and contract bytecode size.\nIn reality, error handling affects not only revert-time Gas usage, but also:\nDeployment cost Baseline execution cost Bytecode size and maintainability This section focuses on a practical and consistently beneficial optimization: efficient error handling and reverts.\n7.1 Reverts Are Not Free When a transaction reverts, state changes are rolled back‚Äîbut Gas is not fully refunded.\nCosts that still apply include:\nGas consumed by executed instructions Data returned with the revert ABI encoding overhead Therefore, frequently triggered validation logic deserves careful cost consideration, even on failure paths.\n7.2 The Real Cost of require(string) The traditional error-handling pattern is:\nrequire(msg.sender == owner, \u0026#34;Not owner\u0026#34;); The problem is not correctness, but cost:\nThe string literal is embedded in bytecode Each revert returns the string data Longer strings increase deployment and revert costs In large contracts, extensive use of require(string) significantly increases bytecode size.\n7.3 Motivation for Custom Errors Solidity 0.8.4 introduced custom errors:\nerror NotOwner(); Used as:\nif (msg.sender != owner) revert NotOwner(); Advantages:\nNo embedded strings Error identifiers encoded as selectors Much smaller revert payloads From a cost perspective, this reduces:\nDeployment Gas Revert-path Gas 7.4 Comparison Example Using require(string):\nfunction withdraw() external { require(msg.sender == owner, \u0026#34;Not owner\u0026#34;); // ... } Using a custom error:\nerror NotOwner(); function withdraw() external { if (msg.sender != owner) revert NotOwner(); // ... } Gas usage on the success path is nearly identical, but differences appear in:\nContract size Revert Gas cost Error encoding efficiency These differences accumulate in complex or high-frequency contracts.\n7.5 How Detailed Should Error Messages Be? A common misconception is:\nMore detailed error messages are always better.\nFrom an on-chain perspective, this is often untrue.\nA better division of responsibility is:\nOn-chain: concise, structured error identifiers Off-chain: documentation or mapping tables explaining errors Custom errors fit this model well because they are:\nStructured Parameterizable Easy for frontends and SDKs to decode Example:\nerror InsufficientBalance(uint256 available, uint256 required); 7.6 Why Bytecode Size Matters Contract bytecode size directly affects:\nDeployment cost Deployment success (there is a size limit) Baseline execution cost (larger code costs more to load) The following increase bytecode size:\nLarge numbers of string literals Repeated logic branches Verbose error messages Gas optimization is therefore not only about runtime execution, but also deployment-time efficiency.\n7.7 Balancing Error Handling and Readability It is important to stress:\nCustom errors are not about extreme compression They do not require sacrificing readability A reasonable approach is:\nUse custom errors for public-facing core interfaces Use require selectively for internal assertions or development checks Avoid embedding long textual descriptions in bytecode 8. Event and Return Value Design In smart contracts, events and function return values are commonly used to expose information externally. However, if designed poorly, they can easily become hidden sources of Gas consumption, especially in high-frequency or data-heavy scenarios.\nThe core idea of this section can be summarized as:\nBlockchains are good at state verification, not data querying.\nUnderstanding this helps you make more economical design choices for events and return values.\n8.1 The Role Boundary of Events The primary purposes of events are:\nAllowing off-chain systems to listen and index Recording important state changes Supporting auditing and analysis Events are not readable on-chain and do not affect subsequent execution logic. From an execution perspective, events are ‚Äúwrite once, off-chain only‚Äù data.\nThis leads to a key design principle:\nEvents should serve off-chain consumers, not replace on-chain state queries.\n8.2 Gas Cost Composition of Events The Gas cost of an event consists of two parts:\nTopics\nThe event signature Up to three indexed parameters Data\nNon-indexed parameters Charged by byte size Intuitively:\nindexed parameters improve filterability But they are not free Larger data payloads cost more Gas Event design therefore requires a trade-off between queryability and cost.\n8.3 Proper Use of indexed Consider a typical transfer event:\nevent Transfer(address indexed from, address indexed to, uint256 amount); This design makes sense because:\nfrom and to are common filter fields amount is rarely used as a filter But if written as:\nevent Transfer( address indexed from, address indexed to, uint256 indexed amount ); Then:\nQuery power barely improves Gas cost increases You quickly hit the three-index limit A practical rule of thumb:\nOnly mark fields as indexed if they are frequently used for filtering.\n8.4 Avoid Carrying Large Data in Events A common but expensive pattern is emitting large data structures:\nevent DataUpdated(uint256[] values); Problems:\nEntire arrays are written to logs Gas cost grows linearly with data size Both on-chain execution and off-chain storage become expensive Better alternatives include:\nEmitting only identifiers, hashes, or counters Storing full data off-chain Linking via hashes or IDs Example:\nevent DataUpdated(bytes32 dataHash); 8.5 Avoid Returning Large Arrays On-Chain Solidity allows functions to return arrays or structs:\nfunction getUsers() external view returns (User[] memory); While intuitive, this design is not cheap.\nWhat Happens in On-Chain Calls If this function is called by another contract, even as a view function, it still consumes Gas. The EVM must:\nRead all User entries from storage Copy them into memory ABI-encode the entire array Return the encoded bytes All of these costs scale linearly with array size‚Äîwith no upper bound.\nWhy This Is Often Unnecessary In real-world projects, full data lists are usually:\nNeeded by frontends or backend services Used for display, analytics, or reporting Not relied upon by other contracts Off-chain systems can retrieve such data for free using eth_call.\nThis creates a common inefficiency:\nOn-chain callers pay Gas for data The actual consumers are off-chain Off-chain consumers could have read the data at zero cost A Better Design Approach Function return values should distinguish between:\nOn-chain callable functions Keep return values minimal or return nothing\nOff-chain query functions Returning arrays or structs is acceptable, but intended for eth_call only\nIf on-chain access is required, pagination or cursor-based APIs are safer.\n8.6 Pagination and Cursor-Based Access For large datasets, pagination is usually preferable:\nfunction getUsers(uint256 offset, uint256 limit) external view returns (User[] memory) { // return a slice } Benefits:\nBounded Gas usage for on-chain calls Efficient off-chain iteration Predictable interface behavior 8.7 Event Design Comparison Example Not recommended:\nevent OrderCreated( address user, uint256[] itemIds, uint256[] prices ); Improved version:\nevent OrderCreated( address indexed user, bytes32 orderId ); With off-chain systems:\nResolving full order data via orderId Treating events as index signals, not data containers This design is far more scalable and cost-effective.\n9. Compact Representation and Low-Level Optimizations (Use with Caution) So far, most optimizations discussed share common traits:\nNo major readability loss Controlled risk Stable, predictable gains This section covers advanced techniques that can save Gas, but also introduce:\nReduced readability Higher implementation complexity Increased audit and maintenance costs The goal here is not to advocate their use, but to answer:\nWhich low-level optimizations are worth using, and under what conditions.\n9.1 Bit Operations and Bitmaps A common and relatively safe advanced technique is the bitmap.\nSuppose you need to track many boolean states:\nWhether an address completed a step Whether an ID is used A fixed-size set of flags The straightforward approach:\nmapping(uint256 =\u0026gt; bool) used; This is clear, but each bool consumes an entire storage slot.\nBitmap Approach If the keys are:\nSequential Bounded Numerous you can pack 256 boolean values into one uint256:\nuint256 bitmap; Example:\nfunction isUsed(uint256 index) internal view returns (bool) { return (bitmap \u0026amp; (1 \u0026lt;\u0026lt; index)) != 0; } function setUsed(uint256 index) internal { bitmap |= (1 \u0026lt;\u0026lt; index); } Benefits:\nOne slot stores 256 flags Far fewer storage reads/writes Significant Gas savings in high-frequency scenarios 9.2 When Bitmaps Make Sense Bitmaps are suitable when:\nThe maximum number of states is known Indices are controlled and not arbitrary user input Logic is stable and unlikely to change They are unsuitable when:\nKeys are addresses or hashes State count is unbounded Readability and flexibility are critical A useful heuristic:\nIf you need documentation just to explain what each bit means, complexity has already increased significantly.\n9.3 Compact Encoding and ‚ÄúSlot-Saving‚Äù Thinking Some projects go further by:\nPacking multiple fields into a single uint256 Using bit shifts and masks manually Re-implementing storage packing logic Example:\nuint256 packed; Where:\nUpper 128 bits store balance Lower 128 bits store timestamp While this can reduce slot count, it also:\nRequires bit manipulation on every access Increases risk of boundary bugs Makes auditing and debugging harder Such techniques are justified only when:\nData structures are extremely stable Access frequency is very high Slot count is confirmed as the main bottleneck 9.4 About assembly Solidity allows inline assembly, enabling direct EVM opcode usage:\nSkipping compiler-generated overhead Achieving lower Gas in extreme cases But be cautious:\nNo type checks No overflow protection Dramatically reduced readability In most business contracts, the risk outweighs the benefit.\n9.5 When assembly Is Worth Using Reasonable scenarios include:\nExtremely hot execution paths Low-level utility functions Logic that is stable and well-tested Teams with audit support and experience Avoid using assembly for:\nCore business logic Permission or fund management Marginal Gas savings A conservative rule:\nIf Gas is already within a reasonable range without assembly, do not use assembly.\n9.6 Evaluating the Real Gains of Advanced Optimizations Advanced optimizations usually yield:\nTens to hundreds of Gas saved per call Meaningful impact only in high-frequency paths Before using them, you should already have:\nCompleted storage, interface, and loop optimizations Identified real bottlenecks Measured Gas with actual test data Otherwise, it is easy to fall into ‚Äúoptimization for its own sake.‚Äù\n10. How to Verify Whether an Optimization Is Effective After discussing many techniques, a more important question emerges:\nHow do you know an optimization is actually worth it?\nThis section emphasizes a data-driven approach rather than intuition.\n10.1 Why You Cannot Rely on Intuition Gas cost does not always correlate with perceived code complexity:\nSome complex refactors barely change Gas Some small changes save a lot Some optimizations matter only at scale Without measurement, it is easy to:\nMiss high-impact improvements Over-optimize low-impact areas Gas optimization must therefore be measured engineering, not guesswork.\n10.2 What Makes an Optimization ‚ÄúEffective‚Äù An optimization should answer three questions:\nHow much Gas does it save? How frequently does this code path execute? How much complexity or risk does it introduce? Only when savings justify complexity is the optimization worthwhile.\n10.3 Basic Benchmarking Approach The simplest and most reliable method is before/after comparison:\nSame input Only one implementation change Compare gas used, not transaction fees Example:\nOriginal function A Optimized function A‚Ä≤ Measure Gas under identical conditions 10.4 A Conceptual Comparison Example Original:\nfunction add(uint256[] calldata xs) external { for (uint256 i = 0; i \u0026lt; xs.length; i++) { total += xs[i]; } } Optimized:\nfunction addOptimized(uint256[] calldata xs) external { uint256 t = total; uint256 len = xs.length; for (uint256 i = 0; i \u0026lt; len; ) { t += xs[i]; unchecked { i++; } } total = t; } The real question is:\nAt xs.length = 10, 100, 1000 Do Gas curves diverge meaningfully? That divergence is what matters.\n10.5 Focus on Worst-Case, Not Just Averages In smart contracts, worst-case behavior often matters more:\nInsufficient Gas causes failure Users hit edge cases Batch and loop risks concentrate at extremes Testing should therefore:\nUse maximum allowed inputs Cover boundary conditions Check proximity to block or function Gas limits 10.6 Tools Matter Less Than Methodology Teams may use:\nHardhat Foundry Truffle Custom scripts The methodology is always the same:\nFix inputs Repeat tests Compare Gas usage Let data drive decisions 10.7 When to Stop Optimizing A frequently overlooked question is: when is enough enough?\nStop when:\nFurther savings are marginal Code complexity increases noticeably High-frequency and high-cost paths are already optimized Audit and maintenance costs outweigh benefits Gas optimization is a balance, not an endless pursuit.\n11. An Actionable Gas Optimization Checklist After systematically covering Gas optimization principles, we can now condense them into a practical checklist for daily development and code reviews.\nThis is not a rigid rulebook, but a priority-driven review order.\n11.1 Design-Phase Checks Before writing code, consider:\nIs this state truly necessary, or derivable? Will the state be frequently accessed? Are there unbounded loops or batch interfaces? Does the data structure have a clear size limit? Are lookup, deduplication, or deletion required? Many Gas issues can be avoided at this stage.\n11.2 Storage Checks (Highest Priority) Are storage variables read multiple times in one function? Is storage written inside loops? Can caching reduce SLOAD / SSTORE? Are variable and struct field orders optimized for packing? Is unused state deleted? Are mappings used instead of arrays for lookup and deduplication? If enumeration is needed, is mapping + array + index used with swap-and-pop? This is where optimization effort pays off the most.\n11.3 Function Interface and Parameter Checks Are external interfaces marked external? Do external functions use calldata? Are unnecessary public functions avoided? Are this calls avoided? Is the external-entry + internal-implementation pattern used? These optimizations are low-risk and consistently effective.\n11.4 Loop and Batch Checks Are array lengths and intermediate values cached? Is storage avoided inside loops? Are O(n) lookups avoided in loops? Is unchecked used safely where applicable? Are loop bounds explicit? Do batch interfaces limit per-call size? Loops amplify Gas‚Äîalways review them from a worst-case perspective.\n11.5 Error Handling and Bytecode Size Are custom errors used instead of require(string)? Are error identifiers concise and structured? Are long strings avoided in bytecode? Is contract size close to deployment limits? These optimizations become more valuable as contracts grow.\n11.6 Events and Return Values Do events only log essential information? Are indexed fields used only for frequent filters? Are large arrays or strings avoided in events? Are large on-chain return values avoided? Are pagination or off-chain indexing used instead? The goal: do not treat the chain as a database.\n11.7 Advanced Optimizations (Use Sparingly) Are foundational optimizations already complete? Is the bottleneck clearly identified? Do bitmaps or packing actually reduce slot usage? Is assembly avoided in core business logic? Advanced optimizations should be data-backed exceptions, not defaults.\n11.8 Let Data Drive Final Decisions Before merging any optimization:\nIs there clear Gas comparison data? Is the optimized path high-frequency or critical? Is the added complexity testable and auditable? If the benefit cannot be clearly explained, the optimization is probably unnecessary.\n","date":"2025-09-20T14:59:56+08:00","permalink":"https://yearsuns-github-io.vercel.app/en/p/best-practices-for-gas-optimization-in-solidity/","title":"Best Practices for Gas Optimization in Solidity"},{"content":"If you know about Uniswap V2, you definitely know that its biggest advantage is ‚Äúfully automated,‚Äù and its biggest problem is also ‚Äúfully automated.‚Äù Once funds enter, they are mechanically distributed across the entire price range from 0 to ‚àû.\nThis is supposed to be fair, but extremely inefficient:\nMost of the time, assets are ‚Äúsleeping‚Äù far away from the current market price, nobody trades there, and no fees are generated.\nTherefore, Uniswap V3 proposed a revolutionary concept: Concentrated Liquidity.\nIt allows LPs to ‚Äúconcentrate‚Äù capital within the price range they consider appropriate so that ‚Äúactive money actually works.‚Äù\nI. What is Concentrated Liquidity: Background and the Problem It Solves The decentralized trading protocol Uniswap uses the automated market maker (AMM) model. Users don‚Äôt need to place orders or find counterparties ‚Äî simply deposit two tokens into a pool to become a liquidity provider. This model is very simple, but it also brings an unnoticed problem: most liquidity does not actually work.\nTo illustrate this, we need to review the mechanism of Uniswap v2.\n1. Traditional AMM: Constant Product and ‚ÄúFull Price Range‚Äù In v2‚Äôs constant product model, a trading pool maintains a formula:\nx ¬∑ y = k\nHere, x and y are the quantities of two assets in the pool, and k is a constant. No matter how the price fluctuates, this formula always holds.\nBut the model implicitly assumes:\nThe liquidity of the pool is distributed across the entire price range from 0 to ‚àû.\nTheoretically, no matter how high or low the price goes, the pool is ‚Äúready‚Äù to provide liquidity.\nThe problem: real prices never go to 0 or infinity.\nIn other words, liquidity is passively spread across a ridiculously wide range that will almost never be used.\n2. Liquidity Utilization Problems Distributing liquidity across an overly wide range results in a very direct consequence:\nMost capital is sleeping.\nIn reality, prices tend to fluctuate within a narrow band, so only funds inside that range actually participate in trades. Other funds are just lying around, consuming capital efficiency.\nThis leads to two impacts:\nFor traders, active depth is limited, causing higher slippage For LPs, although much capital is locked, the return is not proportional In other words:\nCapital utilization is low.\n3. Why Concentrated Liquidity Is Needed Uniswap v3 introduces Concentrated Liquidity, with the following core idea:\nLiquidity should be placed where actual trading happens.\nIf the price stays in a certain region for a long time, LPs can concentrate liquidity in that region instead of wasting it on extreme prices that might never occur.\nThe effects are significant:\nThe same capital provides deeper liquidity Price impact for large orders is reduced Fee revenue increases This essentially changes the AMM from ‚Äúbroad distribution‚Äù to ‚Äúlocally effective distribution.‚Äù\n4. Definition of Concentrated Liquidity Strictly speaking, Concentrated Liquidity means:\nLPs only provide liquidity within a self-defined price range \\([P_{lower}, P_{upper}]\\). Once price moves outside the range, the LP stops participating and stops earning fees.\nThat means each LP is no longer just part of a pool, but becomes a ‚Äúconditional liquidity position.‚Äù\nFrom the contract‚Äôs perspective, the large pool is divided into many segments, and liquidity is placed into each range. Whichever range trades happen within, LPs of that segment earn fees.\n5. What Problem Does It Actually Solve In one sentence:\nUse the same capital to obtain deeper effective liquidity.\nBut specifically, it improves three things:\nCapital Efficiency Lower Slippage Higher Fee Yield These all come from one fact: liquidity can finally be concentrated rather than being infinitely diluted.\n6. But It‚Äôs Not a Free Optimization Concentrated liquidity does not change the basic AMM logic but introduces a new dimension: price range.\nThis makes the LP role more strategic, similar to predicting price movement. Therefore, it also introduces a new risk:\nIf price moves out of the chosen range, the position becomes single-sided and stops earning. To earn more, LPs must take on more management and judgement ‚Äî a typical risk-return tradeoff.\nII. How Concentrated Liquidity Works 1. Inputs Required to Provide Liquidity Suppose we provide liquidity into an ETH/USDC pool, the contract requires:\nToken pair: ETH / USDC\nPrice range\nlower price example: 2000 USDC/ETH upper price example: 3000 USDC/ETH Amount of ETH or USDC provided\nIn other words, we‚Äôre telling the contract three things:\nI want to LP for ETH/USDC Within 2000‚Äì3000 I‚Äôm willing to deposit this amount of tokens 2. Which Token(s) Must Be Deposited Suppose current ETH/USDC price is 2500, then the system checks:\n2000 \u0026lt; 2500 \u0026lt; 3000 Meaning ‚Äúprice is inside the range.‚Äù\nThis is extremely important because it determines which assets we need to provide:\nThree situations Current Price Required Deposit Inside range Provide both ETH and USDC Below range Only USDC Above range Only ETH Why? Not strategy ‚Äî pure math: AMM models automatically bias towards one asset depending on price.\nSimply put, scarcity determines demand. At low price, the pool needs more USDC; at high price, more ETH.\n3. Understanding Liquidity (L) Although we deposit ETH/USDC, the contract records a variable L (liquidity unit).\nKey points:\nWe deposit assets Contract records L Earnings are proportional to L In short:\nL is our ‚Äúposition size‚Äù in that price range.\nThe amounts of ETH/USDC adjust automatically with price.\n4. Price Range Discretization We enter ranges in continuous price (e.g., 2000‚Äì3000), but internally the contract uses ticks.\nIt converts them to:\n\\(tick_{Lower}\\) \\(tick_{Upper}\\) Ticks discretize price boundaries.\nThe contract then stores only boundary updates:\nadd +L at \\(tick_{Lower}\\) subtract ‚ÄìL at \\(tick_{Upper}\\) Active liquidity at current price equals the sum of all netLiquidity values below the current tick.\nFor example, if \\(tick_{Lower}=100\\) and \\(tick_{Upper}=200\\), the contract records:\nnetLiquidity[100] += 10 netLiquidity[200] -= 10 If the current price tick is 150, the activeLiquidity equals the sum of all netLiquidity where tick \u0026lt; 150, which is 10.\nIf the current price tick is 205, then activeLiquidity equals the sum of all netLiquidity where tick \u0026lt; 205, which is 0.\nTherefore, when the price crosses these ticks, the system knows when to start and when to stop using our liquidity.\nWhy do we need to discretize price when storing ranges?\nIf we simply used linear segmentation, such as slicing the 2000‚Äì3000 range into 1000 parts, each worth 1 USDC, it would seem natural.\nBut what if the price range becomes 0.2‚Äì0.3 or even lower? If we still slice it into 1000 segments, each segment would become extremely small, and the overall scale would be highly uneven.\nFurthermore, real token prices can span multiple orders of magnitude, from 0.0000001 to 10,000,000, making uniform segmentation impossible.\nHow do we perform discretization?\nWe define that each tick corresponds to a price that is a fixed multiple \\(r\\) of the previous tick. Then, for \\(tick+1\\), the price \\(P_{tick+1}\\) satisfies:\n\\[ P_{tick+1} = P_{tick} \\times r \\]From which we derive:\n\\[ P_{tick} = P_0 \\times r^{tick} \\]If we set \\(P_0 = 1\\) and \\(r=1.0001\\), then:\n\\[ P_{tick} = 1.0001^{tick} \\]This means each tick movement corresponds to a 0.01% price change.\nNow we can represent any price using ticks:\n\\[ tick = \\log_{1.0001}(P) \\]For example:\n\\(P = 0.25\\) corresponds to \\(tick \\approx -13864\\)\n\\(P = 2500\\) corresponds to \\(tick \\approx 78244\\)\nThus, we obtain a uniform tick scale that works for arbitrary token prices (as long as they‚Äôre not negative ü§™)\n5. Storing Liquidity State When we add liquidity, the system issues an NFT that records:\nchosen price range (2000‚Äì3000) size of L accumulated fees claimable current fee status required for future fee calculations Therefore:\nThe LP action is no longer part of the pool itself, but an independent position recorded as an NFT.\n6. Position Changes When price is within the range:\nholds both ETH and USDC converts between the two as trades occur participates in matching and earns fees If price falls below 2000:\nposition becomes entirely USDC no longer provides liquidity or earns fees If price exceeds 3000:\nposition becomes entirely ETH also exits liquidity In other words:\nWhen ETH rises, the position becomes ETH-heavy; when ETH falls, it becomes USDC-heavy.\nThis behavior is not a strategy ‚Äî it is automatically done by the AMM logic.\n7. Fee Settlement When trades occur inside the range (2500 is inside), fees accumulate. Internally, the contract records fees in the feeGrowth variable.\nWhen withdrawing, the contract does two things:\nreturns current holdings (could be ETH+USDC or single-token) returns accumulated fees No need to manually claim‚Äîwithdrawal automatically settles everything.\nIII. Main Formula Derivations 1. Official Whitepapers v2: https://docs.uniswap.org/whitepaper.pdf\nv3: https://app.uniswap.org/whitepaper-v3.pdf\nv4: https://app.uniswap.org/whitepaper-v4.pdf\n2. Basic Formulas Virtual reserve curve (trading model):\n\\[ x \\times y = k \\tag{1} \\]Liquidity:\n\\[ L = \\sqrt{xy} \\tag{2} \\]Price:\n\\[ P = \\frac{y}{x} \\tag{3} \\]Effective reserve curve:\n\\[ (x+\\dfrac{L}{\\sqrt{p_{b}}})(y+L\\sqrt{p_{a}})=L^{2} \\tag{4} \\]2. Derivation of the Effective Reserve Curve As shown in Figure 1, the current price is at point c. Suppose the actual price movement range is \\([a,b]\\).\nWhen the price moves from c to a, the maximum pool consumption for token \\(y\\) is \\(y_{real}\\). Likewise, when price moves from c to b, the maximum pool consumption for token \\(x\\) is \\(x_{real}\\).\nTherefore, in theory, only \\(x_{real}\\) and \\(y_{real}\\) are required. All other funds are permanently unused, which also explains why the original xyk model has low capital efficiency.\nSo the question is: what should the effective reserve curve actually look like?\nAs shown in Figure 2, we begin deriving the effective reserve curve.\nFrom the following formulas:\n\\[ L^2 = xy \\\\ P = \\frac{y}{x} \\]For anyone who has attended junior high school mathematics, multiplying and dividing both sides of the equations easily yields:\n\\[ \\begin{aligned} x = \\dfrac{L}{\\sqrt{P}} \\\\ y = L\\sqrt{P} \\tag{4} \\end{aligned} \\]From Figure 1 we know:\n\\[ x = x_{real} + x_b \\\\ y = y_{real} + y_a \\]Substituting into the xyk model (1):\n\\[ (x_{real}+x_b)(y_{real}+y_a) = k = L^2 \\]Then substituting \\(x_b, y_a\\) using formula (4):\n\\[ (x_{real}+\\dfrac{L}{\\sqrt{P_{b}}})(y_{real}+L\\sqrt{P_{a}})=L^{2} \\]This is the final formula of the effective reserve curve.\n3. Calculating Liquidity Composition Changes After we provide liquidity, the contract starts performing trades. So how do we calculate the current allocation of assets inside our provided liquidity?\nBased on formula (4):\n\\[ \\begin{aligned} x=\\dfrac{L}{\\sqrt{p}} \\\\ y=L\\sqrt{P} \\tag{4} \\end{aligned} \\]Let:\n\\[ \\begin{aligned} S = \\sqrt{P} \\end{aligned} \\]Then:\n\\[ \\begin{aligned} x = \\dfrac{L}{S} \\\\ y = L S \\end{aligned} \\]Since L remains constant, differentiate with respect to S:\nFor x:\n\\[ x = \\frac{L}{S} \\Rightarrow dx = -L \\frac{1}{S^2} dS \\]For y:\n\\[ y = L S \\Rightarrow dy = L dS \\]So we have two important differential relationships:\n\\[ dx = -L \\frac{1}{S^2} dS,\\qquad dy = L dS \\]Suppose an LP‚Äôs effective price range is:\nLower price: \\(P_a\\), corresponding to \\(S_a = \\sqrt{P_a}\\) Upper price: \\(P_b\\), corresponding to \\(S_b = \\sqrt{P_b}\\) Consider only the \\(token_y\\) change\nInside the range, when S increases from \\(S_a\\) to \\(S_b\\):\n\\[ dy = L dS \\]Integrate S from \\(S_a\\) to \\(S_b\\):\n\\[ \\Delta y = \\int_{S_a}^{S_b} L\\, dS = L (S_b - S_a) = L(\\sqrt{P_b} - \\sqrt{P_a}) \\]This is the total amount of \\(token_y\\) corresponding to the entire interval \\([P_a, P_b]\\):\n\\[ amount_y = L(\\sqrt{P_b} - \\sqrt{P_a}) \\]Consider only \\(token_x\\)\n\\[ dx = -L \\frac{1}{S^2} dS \\]Note that x decreases as S increases (price rises ‚Üí token_x decreases). We want the total amount in absolute value:\n\\[ \\Delta x = \\int_{S_a}^{S_b} L \\frac{1}{S^2} dS \\]Compute:\n\\[ \\int \\frac{1}{S^2} dS = -\\frac{1}{S} \\]Thus:\n\\[ \\Delta x = L\\left[-\\frac{1}{S}\\right]_{S_a}^{S_b} = L\\left(-\\frac{1}{S_b} + \\frac{1}{S_a}\\right) = L\\left(\\frac{1}{S_a} - \\frac{1}{S_b}\\right) \\]Substitute \\(S = \\sqrt{P}\\):\n\\[ amount_x = L\\left(\\frac{1}{\\sqrt{P_a}} - \\frac{1}{\\sqrt{P_b}}\\right) \\]This is the interval formula for \\(token_x\\).\nThe above derivation covers total x/y in the entire interval \\([P_a, P_b]\\). But the actual situation depends on where the current price lies:\nCase A: Current price inside the range \\((P_a \u003c P \u003c P_b)\\)\nThe position holds ‚Äúpart token0 and part token1‚Äù:\nFor \\(token_x\\), effective interval is [P, P_b]:\n\\[ amount_x = L\\left(\\frac{1}{\\sqrt{P}} - \\frac{1}{\\sqrt{P_b}}\\right) \\] For \\(token_y\\), effective interval is [P_a, P]:\n\\[ amount_y = L(\\sqrt{P} - \\sqrt{P_a}) \\] Case B: Price below the range \\((P \\le P_a)\\)\nThe position has not ‚Äúmoved upward yet,‚Äù and is entirely in \\(token_x\\):\n\\[ amount_x = L\\left(\\frac{1}{\\sqrt{P_a}} - \\frac{1}{\\sqrt{P_b}}\\right) \\\\ amount_y = 0 \\]Case C: Price above the range \\((P \\ge P_b)\\)\nThe position has ‚Äúfully moved upward,‚Äù and is entirely \\(token_y\\):\n\\[ amount_x = 0 \\\\ amount_y = L(\\sqrt{P_b} - \\sqrt{P_a}) \\]4. Calculating the Actual Trading Price Now that we understand the trading principle and core formulas, when preparing to swap tokens through the pool, the assets we input will also affect the ratio between pool assets. Since \\(P=\\dfrac{y}{x}\\) is only a theoretical price, how do we calculate the actual execution price? For example, how many USDC can 1 ETH be swapped for?\nWhen we put \\(\\Delta x\\) ETH into the pool, USDC in the pool decreases by \\(\\Delta y\\), and the corresponding price moves from \\(P_0\\) down to \\(P_1\\). So we have:\n\\[ \\begin{aligned} \\Delta x \u0026= L\\left(\\frac{1}{\\sqrt{P_1}} - \\frac{1}{\\sqrt{P_0}}\\right) = L\\left(\\frac{1}{S_1} - \\frac{1}{S_0}\\right) \\Rightarrow S_1 = \\frac{1}{\\dfrac{\\Delta x}{L} + \\dfrac{1}{S_0}} \\\\ \\Delta y \u0026= L(\\sqrt{P_0} - \\sqrt{P_1}) = L(S_0 - S_1) = L\\left(S_0 - \\frac{1}{\\dfrac{\\Delta x}{L} + \\dfrac{1}{S_0}}\\right) \\end{aligned} \\]where L is the active liquidity in the current price segment.\nIf the trade size is large and crosses multiple ticks, split the swap into segments and apply the same method for each interval, summing all the Œîy values across each segment.\n","date":"2025-08-02T14:59:56+08:00","permalink":"https://yearsuns-github-io.vercel.app/en/p/a-deep-dive-into-the-fundamental-principles-of-uniswaps-concentrated-liquidity/","title":"A Deep Understanding of the Fundamental Principles of Uniswap‚Äôs Concentrated Liquidity"},{"content":" XYK means: put a pair of assets into a ‚Äúpool‚Äù, and require that the product of the two asset amounts always stays equal: x * y = k. All swaps must follow this rule, and therefore price, slippage, yield and risk all naturally come from this formula.\nLet‚Äôs walk through the math step by step.\n1. Traditional Trading: Why Do We Need an Order Book? In traditional exchanges, the core mechanism is the order book:\nA places a buy order at 100 for 1 ETH B places a sell order at 101 for 2 ETH The system matches orders and completes the trade This design has several characteristics:\n1. Requires professional market makers Without someone providing orders, ordinary traders cannot trade Market makers must watch the market and continuously update orders 2. Requires a centralized matching engine Run by the exchange You have to trust the intermediary 3. Ordinary users cannot participate in market-making easily Hard to provide liquidity Most profits go to institutions For blockchain, which emphasizes open participation, this is too centralized.\n2. AMM: Turning Market Making into a Liquidity Pool The innovation of AMM (Automated Market Maker) is transforming trading from order matching into a liquidity pool that anyone can join.\nA liquidity pool holds two assets (e.g., ETH and USDT) and follows:\nx * y = k This is the XYK constant product market maker model.\nWhen someone buys ETH with USDT, ETH decreases and USDT increases, while keeping the product constant.\nAMM replaces the order book:\nno buy/sell orders no professional market makers no centralized matching engine anyone can join So essentially:\nAMM = liquidity pool + mathematical rule\nPrice, slippage and liquidity depth all follow from this.\n3. How Is Price Determined? Example:\nPool state:\nETH x = 100 USDT y = 10000 Then the price is naturally:\nprice = y / x = 10000 / 100 = 100 USDT Key points:\nPrice is not quoted manually Price changes automatically with pool state External arbitrage aligns the pool price to market price So in AMM:\nPrice = asset ratio in pool\n4. How Do Trades Change Price? Initial State x = 100 ETH y = 10000 USDT k = 1,000,000 P‚ÇÄ = 100 A user wants to spend 1000 USDT to buy ETH.\nTransaction rule Must satisfy x * y = k before and after the swap\nAfter the swap User adds:\nŒîy = +1000 y‚ÇÇ = 11000 x‚ÇÇ = 1,000,000 / 11000 ‚âà 90.909 Œîx ‚âà 9.091 Average price ‚âà 110 Final price ‚âà 121\nYou observe:\nAverage paid ‚âà 110 Final price ‚âà 121 Initial price = 100 That feeling of:\n‚ÄúI buy ‚Üí the price jumps!‚Äù\nThat is slippage.\n5. Slippage: Not a Fee, But Price You Push Higher Slippage is not a fee:\nYou moved the price by trading You are literally buying along the curve from cheap ‚Üí expensive. Bigger trades cause more slippage.\n6. Why Bigger Pools Have Lower Slippage? Compare:\nSmall pool: 100 / 10000 Price moves 100 ‚Üí 121\nBig pool: 1000 / 100000 Price moves 100 ‚Üí 102\nConclusion:\nMore liquidity ‚Üí milder price curve ‚Üí lower slippage\nIt\u0026rsquo;s like:\na basin vs. a lake adding a bucket changes the level much more in a basin 7. What Exactly Is Liquidity? Liquidity = actual asset amounts in the pool\nmore liquidity ‚Üí flatter curve ‚Üí stable prices less liquidity ‚Üí steep curve ‚Üí price impact increases Anyone can provide liquidity, therefore:\nMarket-making becomes open\n8. How Do You Join as a Liquidity Provider? Pool state:\nx = 100 ETH y = 10000 USDT Price = 100 USDT/ETH\nTo add liquidity, you must deposit proportionally:\nadd 1 ETH plus 100 USDT Otherwise you change the price.\nYou receive LP tokens representing your share.\nWhen you withdraw, you take a share of the pool, not exactly what you put in.\nThis is important for understanding impermanent loss.\n9. How Do LPs Earn Fees? Each trade pays a small fee (e.g., 0.3%), and this fee:\ndoes NOT go to ‚Äúthe platform‚Äù goes into the pool distributed to LPs Think of it as:\nTraders use your assets, and you charge toll fees\nLP earnings:\ntrading fees token incentives 10. Impermanent Loss LPs earn fees but face impermanent loss.\nExample pool:\n1 ETH + 100 USDT You add:\n1 ETH + 100 USDT Pool:\n2 ETH + 200 USDT (You own 50%) If ETH price falls, the pool becomes:\n4 ETH + 100 USDT You withdraw:\n2 ETH + 50 USDT = 100 USDT value You deposited value: 200 You withdraw value: 100\nYou lose value because:\npool moved toward the depreciating asset arbitrage takes the increasing asset out The pool automatically rebalances toward cheaper assets.\n11. What Problems Does XYK Solve? No order book Price determined algorithmically Anyone can become a market maker Infrastructure becomes public and open 12. Limitations \u0026amp; Evolution XYK drawbacks Large slippage ‚Üí specialized stable-swap curves (Curve) Poor capital efficiency ‚Üí concentrated liquidity (Uniswap v3) Impermanent loss ‚Üí LP bears volatility risk Still:\nXYK is the entrance door to AMM design\nUnderstanding XYK makes everything else easier.\n13. The One Simple Formula Back to:\nx * y = k From this follows:\nprice: P = y / x slippage liquidity depth LP revenue impermanent loss XYK is the F = ma of DeFi:\ndeceptively simple, yet the foundation of decentralized trading.\n","date":"2025-07-16T14:59:56+08:00","permalink":"https://yearsuns-github-io.vercel.app/en/p/a-deep-dive-into-the-fundamental-principles-of-uniswaps-xyk-model/","title":"A Deep Dive into the Fundamental Principles of Uniswap‚Äôs XYK Model"},{"content":"1. Introduction: Passwords Are the Last Line of Defense in the Digital World In the entire ecosystem of internet security, passwords are the most common yet also the most fragile component. Almost all services we use daily‚Äîemail, online banking, social platforms, games, forums‚Äîrely on passwords to authenticate user identity.\nA password is essentially a weak credential:\nIt cannot prove who the user really is‚Äîit only proves that they ‚Äúknow a certain secret.‚Äù Once this secret is leaked, an attacker can completely impersonate you. Modern cyberattacks are frequent and diverse. A single ordinary password leak often results in multiple accounts across different platforms being compromised. To defend against these threats, we must strengthen both the transmission stage and the storage stage.\nThis article builds from fundamental concepts and gradually develops a complete, modern password security system.\n2. Why Are Passwords So Dangerous? At first glance, a password is just a string composed of characters. But the damage caused by a leaked password is far more severe than it seems, mainly for two reasons.\n2.1 Risk 1: Leakage During Transmission From the moment a user enters a password until it reaches the server, the data passes through:\nThe browser Local DNS resolution OS network stack Routers ISP backbone Server load balancers If any part of this chain is compromised, the password could be intercepted.\nIn theory, HTTPS solves the ‚Äúeavesdropping problem.‚Äù But reality is more complex. Common risks include:\nMalware installing fake root certificates (classic MITM strategy) Internal corporate/school networks performing TLS inspection Public Wi-Fi conducting ARP spoofing and re-signing HTTPS certificates Browser extensions injecting malicious scripts to read user input ‚ÄúNetwork optimization‚Äù software adding hidden interception modules Enterprise gateways performing SSL inspection (legal but unsafe) In other words:\nYou cannot assume the user‚Äôs device or network environment is always secure.\nTherefore, there is room to improve the security of transmission.\n2.2 Risk 2: Server-side Leakage Most leaks occur on the server side. Common scenarios include:\nDatabase breaches (most common) Passwords accidentally printed in logs Developers outputting passwords during debugging Unencrypted backups mistakenly exposed to the public Third-party monitoring tools capturing sensitive fields Vulnerabilities (SQL injection, RCE) exposing the database Internal misuse of privileges by operations staff If the server stores plaintext passwords, the consequences are catastrophic:\nAll user passwords are exposed Accounts on other websites are compromised due to password reuse If attackers access a user‚Äôs email, the impact escalates further Therefore, the server must ensure that even if a breach occurs, attackers cannot obtain the real passwords.\n3. How Should Passwords Be Safely Transmitted Over the Internet? 3.1 The Most Straightforward Approach: Send the Password Directly Over HTTPS This is the default method used by most websites today:\nBrowser ‚Äî‚ÄîHTTPS‚Äî‚Äî\u0026gt; Server Advantages:\nSimple Mature Fast Compatible with everything The drawback is:\nIt places all risk on the reliability of TLS.\nIf TLS is compromised by a MITM attack, the password is exposed.\nThus, some propose hashing the password on the client side first.\n4. Front-end Hashing: Risks Reduced and Risks Not Reduced The core idea of front-end hashing is:\nEven if the transmission is intercepted, the attacker does not get the real password.\nAssume:\nP = user password H1 = hash(P) The browser sends H1 instead of P.\n4.1 What Can Front-end Hashing Protect Against? Prevents MITM from obtaining the real password Prevents plaintext password leakage through server logs/monitoring Attackers cannot use H1 to log in to the user‚Äôs accounts on other websites Even if users reuse passwords elsewhere, a breach of your site won‚Äôt cascade to others (very important) The server never touches plaintext passwords (a simple form of zero-knowledge authentication) Front-end hashing provides:\n‚ÄúEven if your website is compromised, attackers still cannot learn users‚Äô real passwords for other services.‚Äù\nTraditional approaches cannot offer this.\n4.2 What Can Front-end Hashing Not Protect Against? Attackers can directly use H1 to log in to your website (you treat it as the credential) Keyloggers / infected devices still capture P Hashes without challenge are vulnerable to replay attacks HTTPS is still required‚Äîhashes themselves must be encrypted In short:\nFront-end hashing reduces damage but does not eliminate risk.\n4.3 Is Front-end Hashing Necessary? In practice, it is an optional but clearly beneficial enhancement.\nWhen implemented properly (with challenges to prevent replay), front-end hashing creates a dual defense:\nTLS ensures transport security Hashing reduces cross-site password reuse impact High-security systems (banks, enterprise platforms, developer services) often use such mechanisms.\n5. How Should Servers Store Passwords? Now to the core of password engineering:\nPasswords must never appear in plaintext‚Äîeven inside the server.\nIndustry standards involve three key elements:\nHash Salt Slow hash 5.1 Hashing: Making It Irreversible A good hashing function must:\nHave extremely low collision probability Change output drastically when input changes slightly (avalanche effect) Make it impossible to reverse the input Servers store:\nH = hash(P) If the database leaks, attackers cannot directly obtain the password.\n5.2 Salt: Preventing Rainbow Tables and Mass Cracking Without salt, users with the same weak passwords produce identical hashes. Attackers can:\nUse precomputed rainbow tables Test common passwords once to crack many accounts By adding a random salt:\nH = hash(P + salt) Now:\nTwo users with ‚Äú123456‚Äù have completely different hashes Rainbow tables become useless Attackers must brute-force each user individually (massively increasing cost) Salt must be:\nLong enough (‚â•16 bytes) Random (CSPRNG) Unique per user Stored in plaintext (not encrypted) 5.3 Slow Hashing: Truly Increasing the Cost of Attacks Algorithms like SHA256 are far too fast. Modern GPUs can compute billions of SHA256 hashes per second.\nMeaning:\nEven with salt, SHA256-stored passwords can often be cracked quickly.\nThus, we must use slow hashing algorithms designed specifically for password storage:\nAlgorithm Characteristics bcrypt Classic, mature scrypt GPU-resistant, high memory use Argon2 (recommended) Winner of password hashing competition; tunable CPU/memory/parallelism Slow hashing is not about ‚Äústronger hashing‚Äù; it\u0026rsquo;s about:\nMaking each guess expensive so attackers cannot perform massive brute-force attacks.\nLogin delays are negligible:\nUsers log in infrequently Each slow-hash costs only tens of milliseconds Attackers cannot scale up computations the same way 6. Dual Protection: Front-end Hash + Back-end Slow Hash Combined:\nP ‚Üí H1 = hash1(P) ‚Üí (transmission) ‚Üí H2 = bcrypt(H1 + salt) Dual-layer benefits:\nMITM capturing H1 cannot use it on other sites Database leaks revealing H2 are still hard to crack Server never touches plaintext passwords Logs and monitoring systems cannot leak P This ‚Äúfront-end hash + back-end slow hash‚Äù model is used in high-security environments to reduce cascade risks.\n7. The Replay Attack Problem of Front-end Hashing and Its Solution If front-end hashing is simply:\nH1 = hash(P) An attacker who intercepts H1 can replay it indefinitely‚Äîalmost identical to stealing the real password.\nSolution: Use a random challenge. 7.1 Complete Workflow User opens login page The server generates a challenge:\nchallenge = random string Browser computes:\nH1 = hash(P + challenge) Server verifies:\nbcrypt(H1 + salt) == stored_hash 7.2 Benefits H1 is different every time (challenge varies) Intercepted H1 cannot be reused MITM value decreases Server does not need to store challenge‚Äîjust verify once and discard 7.3 Common Engineering Issues Challenge must not be cached (use no-cache) Must be long enough (‚â•16 bytes) Front-end must use secure hash (SHA256 or higher) Still requires HTTPS (challenge itself can be intercepted otherwise) 8. Additional Important Practices (Often Overlooked) 8.1 Never Email Users Their Passwords Sending a new password via email is extremely dangerous. Correct process:\nSend a password reset link Link contains a one-time token User must set a new password 8.2 Do Not Allow Weak Passwords Weak passwords are exponentially easier to crack:\n123456 password qwerty User phone numbers User birthdays Use:\nBlacklists of common passwords (top 10k) Length policies (‚â•12 chars) Encouragement of password managers 8.3 Multi-factor Authentication (MFA / 2FA) Passwords are only the first layer. A second factor dramatically reduces attack success rates:\nTOTP (Google Authenticator) SMS codes (weaker, but still useful) Hardware keys (FIDO2 / U2F) 8.4 Account Protection Features Lockouts after multiple failed attempts Suspicious-login email alerts Unusual IP/UA verification steps Recent login activity logs for user review 8.5 Password Breach Checks (HIBP API) Many large websites check whether:\nA user\u0026rsquo;s chosen password appears in known breach databases (like HaveIBeenPwned) This significantly reduces risk from weak password reuse.\n9. Goals of a Modern Password Security System With all steps combined, a modern password system should ensure:\n9.1 Secure Transmission Use HTTPS Optional front-end hashing to prevent cascading leaks Challenge-based anti-replay 9.2 Secure Storage Never store plaintext passwords Unique salt per user Use slow hashing algorithms 9.3 Account Protection Strong password policies MFA Anomaly detection Password leak comparison No sensitive info in logs The final objective:\nEven if the entire server is compromised, attackers still cannot obtain any user\u0026rsquo;s real password.\n","date":"2025-06-29T14:59:56+08:00","permalink":"https://yearsuns-github-io.vercel.app/en/p/how-passwords-should-be-safely-transmitted-and-stored/","title":"How Passwords Should Be Safely Transmitted and Stored"}]